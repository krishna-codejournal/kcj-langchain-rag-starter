{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb8dcfe5",
   "metadata": {},
   "source": [
    "**Understanding Document Structure in LangChain**\n",
    "\n",
    "In LangChain, a `Document` is a structured object that contains two main components:\n",
    "\n",
    "- **Content (`page_content`)**: The actual text or data of the document. This can be any string, such as the contents of a file, a web page, or a database entry.\n",
    "- **Metadata (`metadata`)**: A dictionary of key-value pairs that provides additional context about the document. Common metadata fields include source, author, creation date, page number, and custom tags.\n",
    "\n",
    "This structure enables:\n",
    "- **Traceability**: You can track where each piece of information came from.\n",
    "- **Flexible Querying**: Metadata allows for filtering and searching documents based on attributes.\n",
    "- **Context-Aware Processing**: Metadata can be used to provide context during retrieval and generation tasks, improving the relevance and accuracy of results.\n",
    "\n",
    "**Example:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c903b5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Created!\n",
      "Document Content: This is a sample document. This is a sample docume...\n",
      "Document Content Length: 2700 characters\n",
      "Document Metadata: {'source': 'sample_source.txt', 'page': 1, 'author': 'John Doe', 'date_created': '2023-10-01', 'custom_field': 'any_value'}\n",
      "Document Metadata Keys: ['source', 'page', 'author', 'date_created', 'custom_field']\n"
     ]
    }
   ],
   "source": [
    "# create a simple document\n",
    "from langchain_core.documents import Document\n",
    "doc = Document(\n",
    "    page_content=\"This is a sample document. \" * 100,\n",
    "    metadata={\"source\": \"sample_source.txt\",\n",
    "              \"page\" : 1,\n",
    "              \"author\": \"John Doe\",\n",
    "              \"date_created\": \"2023-10-01\",\n",
    "              \"custom_field\": \"any_value\"\n",
    "              }\n",
    ")\n",
    "print(\"Document Created!\")\n",
    "print(f\"Document Content: {doc.page_content[:50]}...\")  # Print first 50 characters\n",
    "print(f\"Document Content Length: {len(doc.page_content)} characters\")\n",
    "print(f\"Document Metadata: {doc.metadata}\")\n",
    "print(f\"Document Metadata Keys: {list(doc.metadata.keys())}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86afed2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"data/text_files\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46dceadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_texts = {\n",
    "\"data/text_files/rag_intro.txt\": \"\"\" RAG stands for Retrieval-Augmented Generation\n",
    "\n",
    "It is an AI framework that significantly enhances the capabilities of large language models (LLMs) by connecting them to external, authoritative knowledge sources before generating a response. Think of it as giving the LLM an \"open-book\" exam instead of a \"closed-book\" one.\n",
    "\n",
    "Key Concepts\n",
    "\n",
    "Retrieval: The process of searching and pulling relevant documents, data, or snippets from an external knowledge base (like a database, an organization's internal documents, or the public internet).\n",
    "Augmentation: The retrieved information is added to the user's original query as additional context.\n",
    "Generation: The LLM uses this augmented prompt (the query + the external facts) to generate a more accurate, up-to-date, and grounded response.\n",
    "Why is RAG Important? RAG helps to overcome several key limitations of traditional LLMs:\n",
    "\n",
    "Reduces \"Hallucinations\": LLMs sometimes generate plausible-sounding but factually incorrect information because they are making predictions based only on their initial, static training data. RAG grounds the model's answer in verifiable external sources, making the output more factual.\n",
    "Access to Current and Specific Data: Traditional LLMs only know what they were trained on, which can quickly become outdated. RAG allows the model to access real-time, specific, or proprietary information (like a company's latest product manual or a customer's account details) without needing to retrain the entire model.\n",
    "Cost-Effective: It is much faster and cheaper to update the external knowledge base than it is to continuously retrain and fine-tune a massive LLM.\n",
    "Transparency: Many RAG systems can provide citations or source links for the information they use, allowing users to verify the claims and building trust.\n",
    "In essence, RAG acts as a dynamic information layer that keeps the powerful generative abilities of an LLM connected to the most current and relevant facts available. \"\"\",\n",
    "\n",
    "\"data/text_files/ml_intro.txt\": \"\"\" Machine learning is a field of artificial intelligence where algorithms learn from data without being explicitly programmed, allowing them to identify patterns and make predictions or decisions. It involves training models on vast datasets, which learn from patterns and relationships to generate new insights, make predictions, or perform tasks. Unlike static software, machine learning models can improve their performance over time as they are exposed to more data.  \n",
    "How it Works\n",
    "Training Data: Machine learning systems are given large sets of data (training data) that contains both inputs and desired outputs. \n",
    "Algorithms: Algorithms act as the rules that analyze this data, searching for mathematical correlations between the inputs and the expected outputs. \n",
    "Learning Patterns: The algorithm uses this information to identify patterns and relationships within the data. \n",
    "Model Creation: The learned patterns are encapsulated into a model, which can then make predictions or decisions when presented with new, unseen data. \n",
    "Key Aspects\n",
    "Data-Driven: Machine learning is fundamentally about learning from data, which drives its ability to make accurate predictions. \n",
    "Adaptive: Models can continuously get better at their assigned tasks as they are exposed to new data and feedback. \n",
    "Predictive Power: A core function of machine learning is to forecast future outcomes, like stock market movements or recommended video content. \n",
    "A Subset of AI: Machine learning is a specialized form of artificial intelligence, focused on the specific ability of learning from data. \n",
    "Examples\n",
    "Healthcare: Identifying trends in patient data to improve diagnoses and treatments. \n",
    "Personalization: Recommending products or videos based on user history and preferences. \n",
    "Image Recognition: Detecting objects or anomalies (like cancer in CT scans) by learning from image datasets.  \"\"\"\n",
    "}\n",
    "\n",
    "\n",
    "for file_path, content in sample_texts.items():\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fe6fef",
   "metadata": {},
   "source": [
    "## Text Loaders in LangChain\n",
    "\n",
    "Text loaders are essential components in LangChain for ingesting and structuring raw data from various sources. They convert files, directories, or other data sources into standardized `Document` objects, enabling downstream processing and retrieval.\n",
    "\n",
    "### Common Text Loaders\n",
    "\n",
    "- **TextLoader**  \n",
    "    Loads a single text file into a `Document`.  \n",
    "    - **Use Case:** When you want to ingest one file at a time.\n",
    "    - **Example:**  \n",
    "        ```python\n",
    "        loader = TextLoader(\"path/to/file.txt\", encoding=\"utf-8\")\n",
    "        docs = loader.load()\n",
    "        ```\n",
    "\n",
    "- **DirectoryLoader**  \n",
    "    Loads multiple files from a directory, using a specified loader for each file.  \n",
    "    - **Use Case:** Batch ingestion of many files (e.g., all `.txt` files in a folder).\n",
    "    - **Features:**  \n",
    "        - Supports glob patterns for file selection.\n",
    "        - Can use different loader classes for different file types.\n",
    "        - Shows progress for large datasets.\n",
    "    - **Example:**  \n",
    "        ```python\n",
    "        dir_loader = DirectoryLoader(\n",
    "                \"data/text_files\",\n",
    "                glob=\"*.txt\",\n",
    "                loader_cls=TextLoader,\n",
    "                loader_kwargs={\"encoding\": \"utf-8\"},\n",
    "                show_progress=True\n",
    "        )\n",
    "        dir_docs = dir_loader.load()\n",
    "        ```\n",
    "\n",
    "### Loader Output\n",
    "\n",
    "All loaders produce a list of `Document` objects, each containing:\n",
    "- **page_content:** The main text of the document.\n",
    "- **metadata:** Contextual information (e.g., source file path).\n",
    "\n",
    "### Loader Selection Tips\n",
    "\n",
    "- Use **TextLoader** for single files or when you need granular control.\n",
    "- Use **DirectoryLoader** for batch processing and scalability.\n",
    "- Choose loader classes based on file type (e.g., PDFLoader, CSVLoader for non-text files).\n",
    "\n",
    "**Efficient data ingestion starts with the right loader choice, ensuring your pipeline is scalable, consistent, and easy to maintain.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a54e6c",
   "metadata": {},
   "source": [
    "**TextLoader Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08225102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of loaded document: <class 'list'>\n",
      "[Document(metadata={'source': 'data/text_files/rag_intro.txt'}, page_content=' RAG stands for Retrieval-Augmented Generation\\n\\nIt is an AI framework that significantly enhances the capabilities of large language models (LLMs) by connecting them to external, authoritative knowledge sources before generating a response. Think of it as giving the LLM an \"open-book\" exam instead of a \"closed-book\" one.\\n\\nKey Concepts\\n\\nRetrieval: The process of searching and pulling relevant documents, data, or snippets from an external knowledge base (like a database, an organization\\'s internal documents, or the public internet).\\nAugmentation: The retrieved information is added to the user\\'s original query as additional context.\\nGeneration: The LLM uses this augmented prompt (the query + the external facts) to generate a more accurate, up-to-date, and grounded response.\\nWhy is RAG Important? RAG helps to overcome several key limitations of traditional LLMs:\\n\\nReduces \"Hallucinations\": LLMs sometimes generate plausible-sounding but factually incorrect information because they are making predictions based only on their initial, static training data. RAG grounds the model\\'s answer in verifiable external sources, making the output more factual.\\nAccess to Current and Specific Data: Traditional LLMs only know what they were trained on, which can quickly become outdated. RAG allows the model to access real-time, specific, or proprietary information (like a company\\'s latest product manual or a customer\\'s account details) without needing to retrain the entire model.\\nCost-Effective: It is much faster and cheaper to update the external knowledge base than it is to continuously retrain and fine-tune a massive LLM.\\nTransparency: Many RAG systems can provide citations or source links for the information they use, allowing users to verify the claims and building trust.\\nIn essence, RAG acts as a dynamic information layer that keeps the powerful generative abilities of an LLM connected to the most current and relevant facts available. ')]\n",
      "[\n",
      "  {\n",
      "    \"id\": null,\n",
      "    \"metadata\": {\n",
      "      \"source\": \"data/text_files/rag_intro.txt\"\n",
      "    },\n",
      "    \"page_content\": \" RAG stands for Retrieval-Augmented Generation\\n\\nIt is an AI framework that significantly enhances the capabilities of large language models (LLMs) by connecting them to external, authoritative knowledge sources before generating a response. Think of it as giving the LLM an \\\"open-book\\\" exam instead of a \\\"closed-book\\\" one.\\n\\nKey Concepts\\n\\nRetrieval: The process of searching and pulling relevant documents, data, or snippets from an external knowledge base (like a database, an organization's internal documents, or the public internet).\\nAugmentation: The retrieved information is added to the user's original query as additional context.\\nGeneration: The LLM uses this augmented prompt (the query + the external facts) to generate a more accurate, up-to-date, and grounded response.\\nWhy is RAG Important? RAG helps to overcome several key limitations of traditional LLMs:\\n\\nReduces \\\"Hallucinations\\\": LLMs sometimes generate plausible-sounding but factually incorrect information because they are making predictions based only on their initial, static training data. RAG grounds the model's answer in verifiable external sources, making the output more factual.\\nAccess to Current and Specific Data: Traditional LLMs only know what they were trained on, which can quickly become outdated. RAG allows the model to access real-time, specific, or proprietary information (like a company's latest product manual or a customer's account details) without needing to retrain the entire model.\\nCost-Effective: It is much faster and cheaper to update the external knowledge base than it is to continuously retrain and fine-tune a massive LLM.\\nTransparency: Many RAG systems can provide citations or source links for the information they use, allowing users to verify the claims and building trust.\\nIn essence, RAG acts as a dynamic information layer that keeps the powerful generative abilities of an LLM connected to the most current and relevant facts available. \",\n",
      "    \"type\": \"Document\"\n",
      "  }\n",
      "]\n",
      "Number of documents loaded: 1\n",
      "First document content (first 100 chars):  RAG stands for Retrieval-Augmented Generation\n",
      "\n",
      "It is an AI framework that significantly enhances th...\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "import json\n",
    "loader = TextLoader(\"data/text_files/rag_intro.txt\", encoding=\"utf-8\")    \n",
    "docs = loader.load()\n",
    "print(\"Type of loaded document:\", type(docs))\n",
    "print(docs)\n",
    "print(json.dumps([doc.model_dump() for doc in docs], indent=2))  # Pretty-print the list of documents\n",
    "print(f\"Number of documents loaded: {len(docs)}\")\n",
    "print(f\"First document content (first 100 chars): {docs[0].page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877e133c",
   "metadata": {},
   "source": [
    "**DirectoryLoader - Multiple Text Files Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc5bfe84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 215.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of loaded documents from directory: <class 'list'> 2\n",
      "Number of documents loaded from directory: 2\n",
      "\n",
      "Document 1:\n",
      "Type: <class 'langchain_core.documents.base.Document'>\n",
      "Content (first 100 chars):  Machine learning is a field of artificial intelligence where algorithms learn from data without bei...\n",
      "Metadata: {'source': 'data\\\\text_files\\\\ml_intro.txt'}\n",
      "\n",
      "Document 2:\n",
      "Type: <class 'langchain_core.documents.base.Document'>\n",
      "Content (first 100 chars):  RAG stands for Retrieval-Augmented Generation\n",
      "\n",
      "It is an AI framework that significantly enhances th...\n",
      "Metadata: {'source': 'data\\\\text_files\\\\rag_intro.txt'}\n",
      "\n",
      "**📈Advantages of DirectoryLoader**:\n",
      "1. **Batch Processing**: Load multiple files in one go, saving time and effort.\n",
      "2. **Consistency**: Ensures all files are processed using the same loader and settings.\n",
      "3. **Scalability**: Easily handle large volumes of documents by loading them in batches.\n",
      "4. **Flexibility**: Supports various file types and structures through customizable loaders.\n",
      "5. **Progress Tracking**: Provides feedback on loading progress, useful for large datasets. \n",
      "\n",
      "**⚠️Disadvantages of DirectoryLoader**:\n",
      "1. **Limited Control**: Less granular control over individual file processing compared to single-file loaders.\n",
      "2. **Error Propagation**: Errors in one file may affect the loading of subsequent files.\n",
      "3. **Resource Intensive**: Loading many files at once can consume significant memory and processing power.\n",
      "4. **Complexity**: May introduce complexity in managing and configuring multiple loaders for different file types.\n",
      "5. **Debugging Challenges**: Harder to debug issues with specific files when using batch loading. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "dir_loader = DirectoryLoader(\"data/text_files\", \n",
    "                             glob=\"*.txt\",  # Pattern to match all .txt files in the directory and subdirectories\n",
    "                             loader_cls=TextLoader, # Specify the loader class to use for each file\n",
    "                            loader_kwargs={\"encoding\": \"utf-8\"},\n",
    "                            show_progress=True\n",
    "                            )\n",
    "dir_docs = dir_loader.load()\n",
    "print(\"Type of loaded documents from directory:\", type(dir_docs) , len(dir_docs))\n",
    "print(f\"Number of documents loaded from directory: {len(dir_docs)}\")\n",
    "\n",
    "for i, doc in enumerate(dir_docs): # i means index, doc means document\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(f\"Type: {type(doc)}\")\n",
    "    print(f\"Content (first 100 chars): {doc.page_content[:100]}...\")\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "    if i >= 2:  # Limit output to first 3 documents for brevity\n",
    "        break\n",
    " \n",
    "print(\"\"\"\\n**📈Advantages of DirectoryLoader**:\n",
    "1. **Batch Processing**: Load multiple files in one go, saving time and effort.\n",
    "2. **Consistency**: Ensures all files are processed using the same loader and settings.\n",
    "3. **Scalability**: Easily handle large volumes of documents by loading them in batches.\n",
    "4. **Flexibility**: Supports various file types and structures through customizable loaders.\n",
    "5. **Progress Tracking**: Provides feedback on loading progress, useful for large datasets. \"\"\")\n",
    "\n",
    "# disadvantages of DirectoryLoader\n",
    "print(\"\"\"\\n**⚠️Disadvantages of DirectoryLoader**:\n",
    "1. **Limited Control**: Less granular control over individual file processing compared to single-file loaders.\n",
    "2. **Error Propagation**: Errors in one file may affect the loading of subsequent files.\n",
    "3. **Resource Intensive**: Loading many files at once can consume significant memory and processing power.\n",
    "4. **Complexity**: May introduce complexity in managing and configuring multiple loaders for different file types.\n",
    "5. **Debugging Challenges**: Harder to debug issues with specific files when using batch loading. \"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac861ed",
   "metadata": {},
   "source": [
    "## Text Splitters and Strategies in LangChain\n",
    "\n",
    "Text splitters are essential tools in LangChain for dividing large texts into manageable, context-preserving chunks. This process is crucial for tasks like retrieval, embedding, and generation, where input size limits and context windows must be respected.\n",
    "\n",
    "### Why Split Text?\n",
    "\n",
    "- **Model Constraints**: LLMs and embedding models have maximum input sizes (characters or tokens).\n",
    "- **Efficient Retrieval**: Smaller chunks improve search relevance and granularity.\n",
    "- **Context Preservation**: Overlapping chunks help maintain continuity across splits.\n",
    "\n",
    "### Common Text Splitting Strategies\n",
    "\n",
    "1. **Character-Based Splitting**\n",
    "    - **CharacterTextSplitter**: Splits text at specified characters or separators (e.g., newline, period).\n",
    "    - **Use Case**: Structured text with clear delimiters (paragraphs, lines).\n",
    "    - **Parameters**: `separator`, `chunk_size`, `chunk_overlap`.\n",
    "\n",
    "2. **Recursive Splitting**\n",
    "    - **RecursiveCharacterTextSplitter**: Tries multiple separators in order, recursively splitting to best fit chunk size.\n",
    "    - **Use Case**: Unstructured or complex text, produces more balanced chunks.\n",
    "    - **Parameters**: `separators` (list), `chunk_size`, `chunk_overlap`.\n",
    "\n",
    "3. **Token-Based Splitting**\n",
    "    - **TokenTextSplitter**: Splits text based on token count using a tokenizer.\n",
    "    - **Use Case**: NLP tasks and LLMs where token limits matter (e.g., OpenAI models).\n",
    "    - **Parameters**: `chunk_size` (tokens), `chunk_overlap` (tokens).\n",
    "\n",
    "### Choosing a Strategy\n",
    "\n",
    "- **Simple, Structured Text**: Use `CharacterTextSplitter`.\n",
    "- **Complex or Mixed Text**: Use `RecursiveCharacterTextSplitter` for balanced chunks.\n",
    "- **Token-Limited Models**: Use `TokenTextSplitter` for precise control over input size.\n",
    "\n",
    "**Tip:** Always consider the downstream task (retrieval, embedding, generation) and the model's input constraints when selecting a splitter and configuring its parameters.\n",
    "\n",
    "Text splitting is a foundational step for building robust, scalable, and context-aware AI pipelines in LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d65c7bc",
   "metadata": {},
   "source": [
    "**Method 1: CharacterTextSplitter splitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3677c9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'data/text_files/rag_intro.txt'}, page_content=' RAG stands for Retrieval-Augmented Generation\\n\\nIt is an AI framework that significantly enhances the capabilities of large language models (LLMs) by connecting them to external, authoritative knowledge sources before generating a response. Think of it as giving the LLM an \"open-book\" exam instead of a \"closed-book\" one.\\n\\nKey Concepts\\n\\nRetrieval: The process of searching and pulling relevant documents, data, or snippets from an external knowledge base (like a database, an organization\\'s internal documents, or the public internet).\\nAugmentation: The retrieved information is added to the user\\'s original query as additional context.\\nGeneration: The LLM uses this augmented prompt (the query + the external facts) to generate a more accurate, up-to-date, and grounded response.\\nWhy is RAG Important? RAG helps to overcome several key limitations of traditional LLMs:\\n\\nReduces \"Hallucinations\": LLMs sometimes generate plausible-sounding but factually incorrect information because they are making predictions based only on their initial, static training data. RAG grounds the model\\'s answer in verifiable external sources, making the output more factual.\\nAccess to Current and Specific Data: Traditional LLMs only know what they were trained on, which can quickly become outdated. RAG allows the model to access real-time, specific, or proprietary information (like a company\\'s latest product manual or a customer\\'s account details) without needing to retrain the entire model.\\nCost-Effective: It is much faster and cheaper to update the external knowledge base than it is to continuously retrain and fine-tune a massive LLM.\\nTransparency: Many RAG systems can provide citations or source links for the information they use, allowing users to verify the claims and building trust.\\nIn essence, RAG acts as a dynamic information layer that keeps the powerful generative abilities of an LLM connected to the most current and relevant facts available. ')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' RAG stands for Retrieval-Augmented Generation\\n\\nIt is an AI framework that significantly enhances the capabilities of large language models (LLMs) by connecting them to external, authoritative knowledge sources before generating a response. Think of it as giving the LLM an \"open-book\" exam instead of a \"closed-book\" one.\\n\\nKey Concepts\\n\\nRetrieval: The process of searching and pulling relevant documents, data, or snippets from an external knowledge base (like a database, an organization\\'s internal documents, or the public internet).\\nAugmentation: The retrieved information is added to the user\\'s original query as additional context.\\nGeneration: The LLM uses this augmented prompt (the query + the external facts) to generate a more accurate, up-to-date, and grounded response.\\nWhy is RAG Important? RAG helps to overcome several key limitations of traditional LLMs:\\n\\nReduces \"Hallucinations\": LLMs sometimes generate plausible-sounding but factually incorrect information because they are making predictions based only on their initial, static training data. RAG grounds the model\\'s answer in verifiable external sources, making the output more factual.\\nAccess to Current and Specific Data: Traditional LLMs only know what they were trained on, which can quickly become outdated. RAG allows the model to access real-time, specific, or proprietary information (like a company\\'s latest product manual or a customer\\'s account details) without needing to retrain the entire model.\\nCost-Effective: It is much faster and cheaper to update the external knowledge base than it is to continuously retrain and fine-tune a massive LLM.\\nTransparency: Many RAG systems can provide citations or source links for the information they use, allowing users to verify the claims and building trust.\\nIn essence, RAG acts as a dynamic information layer that keeps the powerful generative abilities of an LLM connected to the most current and relevant facts available. '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import(\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    CharacterTextSplitter,\n",
    "    TokenTextSplitter\n",
    ")  \n",
    "print(docs)\n",
    "text = docs[0].page_content\n",
    "text\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2f256e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 274, which is longer than the specified 100\n",
      "Created a chunk of size 198, which is longer than the specified 100\n",
      "Created a chunk of size 143, which is longer than the specified 100\n",
      "Created a chunk of size 286, which is longer than the specified 100\n",
      "Created a chunk of size 322, which is longer than the specified 100\n",
      "Created a chunk of size 147, which is longer than the specified 100\n",
      "Created a chunk of size 154, which is longer than the specified 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of character-based chunks: 12\n",
      "\n",
      "Character Chunk 1 (length 45):\n",
      "RAG stands for Retrieval-Augmented Generation\n",
      "\n",
      "Character Chunk 2 (length 274):\n",
      "It is an AI framework that significantly enhances the capabilities of large language models (LLMs) by connecting them to external, authoritative knowledge sources before generating a response. Think of it as giving the LLM an \"open-book\" exam instead of a \"closed-book\" one.\n",
      "\n",
      "Character Chunk 3 (length 12):\n",
      "Key Concepts\n",
      "\n",
      "---\n",
      "\n",
      "['RAG stands for Retrieval-Augmented Generation', 'It is an AI framework that significantly enhances the capabilities of large language models (LLMs) by connecting them to external, authoritative knowledge sources before generating a response. Think of it as giving the LLM an \"open-book\" exam instead of a \"closed-book\" one.', 'Key Concepts', \"Retrieval: The process of searching and pulling relevant documents, data, or snippets from an external knowledge base (like a database, an organization's internal documents, or the public internet).\", \"Augmentation: The retrieved information is added to the user's original query as additional context.\", 'Generation: The LLM uses this augmented prompt (the query + the external facts) to generate a more accurate, up-to-date, and grounded response.', 'Why is RAG Important? RAG helps to overcome several key limitations of traditional LLMs:', 'Reduces \"Hallucinations\": LLMs sometimes generate plausible-sounding but factually incorrect information because they are making predictions based only on their initial, static training data. RAG grounds the model\\'s answer in verifiable external sources, making the output more factual.', \"Access to Current and Specific Data: Traditional LLMs only know what they were trained on, which can quickly become outdated. RAG allows the model to access real-time, specific, or proprietary information (like a company's latest product manual or a customer's account details) without needing to retrain the entire model.\", 'Cost-Effective: It is much faster and cheaper to update the external knowledge base than it is to continuously retrain and fine-tune a massive LLM.', 'Transparency: Many RAG systems can provide citations or source links for the information they use, allowing users to verify the claims and building trust.', 'In essence, RAG acts as a dynamic information layer that keeps the powerful generative abilities of an LLM connected to the most current and relevant facts available.']\n",
      "RAG stands for Retrieval-Augmented Generation\n",
      "It is an AI framework that significantly enhances the capabilities of large language models (LLMs) by connecting them to external, authoritative knowledge sources before generating a response. Think of it as giving the LLM an \"open-book\" exam instead of a \"closed-book\" one.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "char_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\", # split at new lines\n",
    "    chunk_size=100, # each chunk will be 100 characters\n",
    "    chunk_overlap=20, # 20 characters overlap between chunks\n",
    "    length_function=len # function to measure length (default is len)\n",
    ")\n",
    "\n",
    "char_chunks = char_splitter.split_text(text)\n",
    "print(f\"Number of character-based chunks: {len(char_chunks)}\")\n",
    "for i, chunk in enumerate(char_chunks[:3]):  # Print first 3 chunks\n",
    "    print(f\"\\nCharacter Chunk {i+1} (length {len(chunk)}):\\n{chunk}\")\n",
    "print(\"\\n---\\n\")\n",
    "print(char_chunks)\n",
    "print(char_chunks[0])\n",
    "print(char_chunks[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaaaf12",
   "metadata": {},
   "source": [
    "**Warning : The chunk is larger than the specified chunk_size=100 because of how CharacterTextSplitter works with the separator parameter:**\n",
    "\n",
    "Separator Behavior: When you specify separator=\"\\n\", the splitter will only split at newline characters. It won't split in the middle of a paragraph or line, even if the chunk size exceeds the specified limit.\n",
    "\n",
    "Chunk Size is a Target: The chunk_size of 100 is treated as a target/minimum size, not a strict maximum. If a text segment between separators is longer than the chunk size, it will be kept intact.\n",
    "\n",
    "To fix this and get smaller chunks, you have two options:\n",
    "\n",
    "\n",
    "1. Use Different Separators:   <br>\n",
    "char_splitter = CharacterTextSplitter(  <br>\n",
    "    separator=[\"\\n\", \".\", \" \"],  # Try multiple separators   <br>\n",
    "    chunk_size=100,<br>\n",
    "    chunk_overlap=20,<br>\n",
    "    length_function=len<br>\n",
    ")\n",
    "<br>\n",
    "\n",
    "2.Use RecursiveCharacterTextSplitter: The RecursiveCharacterTextSplitter is generally recommended as it handles this situation better by trying different separators in order: [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(<br>\n",
    "    chunk_size=100,<br>\n",
    "    chunk_overlap=20,<br>\n",
    "    length_function=len<br>\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bddd27",
   "metadata": {},
   "source": [
    "**Method 2 : Recursive Character Text Splitter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "038540d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of recursive chunks: 15\n",
      "\n",
      "Recursive Chunk 1 (length 45):\n",
      "RAG stands for Retrieval-Augmented Generation\n",
      "\n",
      "Recursive Chunk 2 (length 198):\n",
      "It is an AI framework that significantly enhances the capabilities of large language models (LLMs) by connecting them to external, authoritative knowledge sources before generating a response. Think\n",
      "\n",
      "Recursive Chunk 3 (length 93):\n",
      "a response. Think of it as giving the LLM an \"open-book\" exam instead of a \"closed-book\" one.\n",
      "\n",
      "---\n",
      "\n",
      "['RAG stands for Retrieval-Augmented Generation', 'It is an AI framework that significantly enhances the capabilities of large language models (LLMs) by connecting them to external, authoritative knowledge sources before generating a response. Think', 'a response. Think of it as giving the LLM an \"open-book\" exam instead of a \"closed-book\" one.', 'Key Concepts', \"Retrieval: The process of searching and pulling relevant documents, data, or snippets from an external knowledge base (like a database, an organization's internal documents, or the public internet).\", \"Augmentation: The retrieved information is added to the user's original query as additional context.\", 'Generation: The LLM uses this augmented prompt (the query + the external facts) to generate a more accurate, up-to-date, and grounded response.', 'Why is RAG Important? RAG helps to overcome several key limitations of traditional LLMs:', 'Reduces \"Hallucinations\": LLMs sometimes generate plausible-sounding but factually incorrect information because they are making predictions based only on their initial, static training data. RAG', \"training data. RAG grounds the model's answer in verifiable external sources, making the output more factual.\", 'Access to Current and Specific Data: Traditional LLMs only know what they were trained on, which can quickly become outdated. RAG allows the model to access real-time, specific, or proprietary', \"or proprietary information (like a company's latest product manual or a customer's account details) without needing to retrain the entire model.\", 'Cost-Effective: It is much faster and cheaper to update the external knowledge base than it is to continuously retrain and fine-tune a massive LLM.', 'Transparency: Many RAG systems can provide citations or source links for the information they use, allowing users to verify the claims and building trust.', 'In essence, RAG acts as a dynamic information layer that keeps the powerful generative abilities of an LLM connected to the most current and relevant facts available.']\n",
      "RAG stands for Retrieval-Augmented Generation\n",
      "It is an AI framework that significantly enhances the capabilities of large language models (LLMs) by connecting them to external, authoritative knowledge sources before generating a response. Think\n"
     ]
    }
   ],
   "source": [
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators= [\"\\n\\n\",\"\\n\",\" \",\"\"],  # Try this separators in order of priority\n",
    "    chunk_size=200, # each chunk will be 200 characters\n",
    "    chunk_overlap=20, # 20 characters overlap between chunks\n",
    "    length_function=len # function to measure length (default is len)\n",
    ")\n",
    "\n",
    "rec_chunks = recursive_splitter.split_text(text)\n",
    "print(f\"Number of recursive chunks: {len(rec_chunks)}\")\n",
    "for i, chunk in enumerate(rec_chunks[:3]):  # Print first 3 chunks\n",
    "    print(f\"\\nRecursive Chunk {i+1} (length {len(chunk)}):\\n{chunk}\")\n",
    "print(\"\\n---\\n\")\n",
    "print(rec_chunks)\n",
    "print(rec_chunks[0])\n",
    "print(rec_chunks[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "656672c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sample recursive chunks : 3\n",
      "\n",
      "---\n",
      "\n",
      "This is RAG framework. It connects LLMs to\n",
      "\n",
      "Sample Recursive Chunk 1 (length 42):\n",
      "This is RAG framework. It connects LLMs to\n",
      "\n",
      "Sample Recursive Chunk 2 (length 48):\n",
      "LLMs to external knowledge sources. RAG enhances\n",
      "\n",
      "Sample Recursive Chunk 3 (length 45):\n",
      "enhances accuracy and reduces hallucinations.\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"This is RAG framework. It connects LLMs to external knowledge sources. RAG enhances accuracy and reduces hallucinations.\"\n",
    "sample_recursive_chunks = RecursiveCharacterTextSplitter(\n",
    "    separators=[\" \"],\n",
    "    chunk_size=50,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "sample_chunks = sample_recursive_chunks.split_text(sample_text)\n",
    "print(f\"Number of sample recursive chunks : {len(sample_chunks)}\")\n",
    "print(\"\\n---\\n\")\n",
    "print(sample_chunks[0])\n",
    "\n",
    "for i, chunk in enumerate(sample_chunks):  # Print all sample chunks\n",
    "    print(f\"\\nSample Recursive Chunk {i+1} (length {len(chunk)}):\\n{chunk}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b073b57",
   "metadata": {},
   "source": [
    "**Method 3 : Token based Splitter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df21dba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of token-based chunks: 5\n",
      "\n",
      "Token Chunk 1 (length 461):\n",
      " RAG stands for Retrieval-Augmented Generation\n",
      "\n",
      "It is an AI framework that significantly enhances the capabilities of large language models (LLMs) by connecting them to external, authoritative knowledge sources before generating a response. Think of it as giving the LLM an \"open-book\" exam instead of a \"closed-book\" one.\n",
      "\n",
      "Key Concepts\n",
      "\n",
      "Retrieval: The process of searching and pulling relevant documents, data, or snippets from an external knowledge base (like\n",
      "\n",
      "Token Chunk 2 (length 462):\n",
      ", or snippets from an external knowledge base (like a database, an organization's internal documents, or the public internet).\n",
      "Augmentation: The retrieved information is added to the user's original query as additional context.\n",
      "Generation: The LLM uses this augmented prompt (the query + the external facts) to generate a more accurate, up-to-date, and grounded response.\n",
      "Why is RAG Important? RAG helps to overcome several key limitations of traditional LLMs:\n",
      "\n",
      "\n",
      "\n",
      "Token Chunk 3 (length 510):\n",
      " several key limitations of traditional LLMs:\n",
      "\n",
      "Reduces \"Hallucinations\": LLMs sometimes generate plausible-sounding but factually incorrect information because they are making predictions based only on their initial, static training data. RAG grounds the model's answer in verifiable external sources, making the output more factual.\n",
      "Access to Current and Specific Data: Traditional LLMs only know what they were trained on, which can quickly become outdated. RAG allows the model to access real-time, specific\n"
     ]
    }
   ],
   "source": [
    "token_splitter = TokenTextSplitter(\n",
    "    chunk_size=100, # each chunk will be 100 tokens\n",
    "    chunk_overlap=10, # 10 tokens overlap between chunks\n",
    "    length_function=len\n",
    ")\n",
    "token_chunks = token_splitter.split_text(text)\n",
    "print(f\"Number of token-based chunks: {len(token_chunks)}\")\n",
    "for i, chunk in enumerate(token_chunks[:3]):  # Print first 3 token chunks\n",
    "    print(f\"\\nToken Chunk {i+1} (length {len(chunk)}):\\n{chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84693d89",
   "metadata": {},
   "source": [
    "### Differences Between TextSplitter, RecursiveCharacterTextSplitter, and TokenTextSplitter\n",
    "\n",
    "| Splitter Type                      | Splitting Strategy                | Use Case / Strengths                                   | Parameters & Customization                |\n",
    "|------------------------------------|-----------------------------------|--------------------------------------------------------|-------------------------------------------|\n",
    "| **CharacterTextSplitter**          | Splits text at specified character(s) or separators (e.g., newline, period, space). | Simple, fast splitting for structured text (e.g., paragraphs, lines). | `separator`, `chunk_size`, `chunk_overlap`, `length_function` |\n",
    "| **RecursiveCharacterTextSplitter** | Tries multiple separators in order of priority, recursively splitting to best fit chunk size. | Handles unstructured or complex text, produces more balanced chunk sizes. | `separators` (list), `chunk_size`, `chunk_overlap`, `length_function` |\n",
    "| **TokenTextSplitter**              | Splits text based on token count (not character count), using a tokenizer. | Useful for LLMs and NLP tasks where token limits matter (e.g., OpenAI models). | `chunk_size` (tokens), `chunk_overlap` (tokens), `length_function` |\n",
    "\n",
    "**Summary:**\n",
    "- **CharacterTextSplitter**: Best for simple, character-based splitting. Structed text, Text has clear delimiters. Fastest option. May break chunks unevenly if text lacks separators.\n",
    "- **RecursiveCharacterTextSplitter**: Best for robust, multi-separator splitting and balanced chunks. Default choice for varied text. Slightly slower due to recursion.\n",
    "- **TokenTextSplitter**: Best for token-aware splitting, ideal for LLM input constraints. More accurate for embeddings and model inputs. Slower due to tokenization overhead."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kcj-langchain-rag-starter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
